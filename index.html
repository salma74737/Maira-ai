<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MAIRA AI Voice Assistant</title>
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;600&display=swap');

        :root {
            --background-color: #1a1a1d;
            --surface-color: #2c2c34;
            --primary-color: #8a2be2; /* A rich purple */
            --secondary-color: #4a90e2; /* A calm blue */
            --text-color: #f0f0f5;
            --shadow-color: rgba(138, 43, 226, 0.2);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Poppins', sans-serif;
            background-color: var(--background-color);
            color: var(--text-color);
            display: flex;
            justify-content: center;
            align-items: center;
            min-height: 100vh;
            overflow: hidden;
        }

        .container {
            width: 90%;
            max-width: 800px;
            height: 90vh;
            max-height: 700px;
            background-color: var(--surface-color);
            border-radius: 20px;
            box-shadow: 0 10px 40px rgba(0, 0, 0, 0.4);
            display: flex;
            flex-direction: column;
            overflow: hidden;
        }

        .header {
            text-align: center;
            padding: 1.5rem;
            border-bottom: 1px solid rgba(255, 255, 255, 0.1);
        }

        .header h1 {
            font-size: 2rem;
            font-weight: 600;
            color: var(--text-color);
            letter-spacing: 1px;
        }

        .main-content {
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            justify-content: center;
            align-items: center;
            padding: 2rem;
            gap: 2rem;
        }

        #visualizer {
            width: 150px;
            height: 150px;
            border-radius: 50%;
            background: radial-gradient(circle, var(--primary-color) 0%, var(--surface-color) 70%);
            display: flex;
            justify-content: center;
            align-items: center;
            transition: transform 0.3s ease, box-shadow 0.3s ease;
        }
        
        #visualizer.listening {
            animation: pulse 1.5s infinite ease-in-out;
            box-shadow: 0 0 30px var(--shadow-color), 0 0 50px var(--shadow-color);
        }
        
        #visualizer.speaking {
            animation: speak-wave 1.2s infinite ease-in-out;
             box-shadow: 0 0 40px var(--shadow-color), 0 0 70px var(--shadow-color);
        }
        
        @keyframes pulse {
            0% { transform: scale(0.95); }
            70% { transform: scale(1.05); }
            100% { transform: scale(0.95); }
        }
        
        @keyframes speak-wave {
             0% { background-size: 100% 100%; }
             50% { background-size: 150% 150%; }
             100% { background-size: 100% 100%; }
        }


        #status {
            font-size: 1.1rem;
            font-weight: 300;
            height: 25px;
            color: var(--text-color);
            opacity: 0.8;
            transition: color 0.3s ease;
        }

        #conversation-log {
            width: 100%;
            height: 150px;
            overflow-y: auto;
            border-top: 1px solid rgba(255, 255, 255, 0.1);
            padding: 1.5rem;
            display: flex;
            flex-direction: column;
            gap: 1rem;
        }
        
        /* Custom scrollbar for webkit browsers */
        #conversation-log::-webkit-scrollbar {
            width: 6px;
        }
        #conversation-log::-webkit-scrollbar-track {
            background: transparent;
        }
        #conversation-log::-webkit-scrollbar-thumb {
            background: var(--primary-color);
            border-radius: 3px;
        }

        .log-entry {
            max-width: 80%;
            padding: 0.75rem 1rem;
            border-radius: 15px;
            line-height: 1.5;
        }

        .user-message {
            background-color: var(--secondary-color);
            align-self: flex-end;
            border-bottom-right-radius: 5px;
        }

        .maira-message {
            background-color: #3a3a42;
            align-self: flex-start;
            border-bottom-left-radius: 5px;
        }
    </style>
</head>
<body>

    <div class="container">
        <div class="header">
            <h1>MAIRA AI</h1>
        </div>
        <div class="main-content">
            <div id="visualizer"></div>
            <p id="status">Initializing system...</p>
        </div>
        <div id="conversation-log">
            <div class="log-entry maira-message">System online. How can I assist you?</div>
        </div>
    </div>

    <script>
        const statusEl = document.getElementById('status');
        const visualizerEl = document.getElementById('visualizer');
        const logEl = document.getElementById('conversation-log');
        const openRouterApiKey = 'YOUR_OPENROUTER_API_KEY'; // IMPORTANT: Replace with your OpenRouter API key

        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
        let recognition;
        let isListening = false;
        let synth = window.speechSynthesis;
        let maleVoice = null;
        let lastUtterance = null; // To track the last spoken sentence

        function initializeMaira() {
            if (!('speechSynthesis' in window)) {
                updateStatus("Speech synthesis not supported", true);
                return;
            }
             if (!SpeechRecognition) {
                updateStatus("Speech recognition not supported", true);
                return;
            }

            loadVoices();
            if (synth.onvoiceschanged !== undefined) {
                synth.onvoiceschanged = loadVoices;
            }
            
            setupRecognition();
            startListening();
        }

        function loadVoices() {
            const voices = synth.getVoices();
            // Voice selection priority
            maleVoice = voices.find(voice => voice.lang === 'en-IN' && voice.name.toLowerCase().includes('male')) ||
                        voices.find(voice => voice.lang === 'en-US' && voice.name === 'Google US English') ||
                        voices.find(voice => voice.lang === 'hi-IN' && voice.name.toLowerCase().includes('male')) ||
                        voices.find(voice => voice.lang.startsWith('en-') && voice.name.toLowerCase().includes('male')) ||
                        voices.find(voice => voice.lang.startsWith('en-') && voice.default); // Fallback to default English

             if(!maleVoice && voices.length > 0){
                maleVoice = voices.find(voice => voice.lang.startsWith('en-')); // Broader fallback
            }

            if(maleVoice){
                updateStatus("Ready for your command.");
            } else {
                updateStatus("Could not find a suitable voice.", true);
                 console.log("Available voices:", voices);
            }
        }
        
        function setupRecognition() {
            recognition = new SpeechRecognition();
            recognition.continuous = true; // Listen continuously
            recognition.interimResults = false;
            recognition.lang = 'en-US';

            recognition.onstart = () => {
                isListening = true;
                visualizerEl.classList.add('listening');
                updateStatus("I'm listening...");
            };

            recognition.onend = () => {
                isListening = false;
                visualizerEl.classList.remove('listening');
                updateStatus("Listening paused.");
                // Automatically restart if not currently speaking
                if(!synth.speaking){
                    startListening();
                }
            };

            recognition.onerror = (event) => {
                console.error("Speech Recognition Error:", event.error);
                let errorMsg = "An error occurred.";
                if (event.error === 'no-speech') errorMsg = "I didn't hear anything.";
                if (event.error === 'audio-capture') errorMsg = "Check microphone permissions.";
                if (event.error === 'not-allowed') errorMsg = "Microphone access denied.";
                updateStatus(errorMsg, true);
            };

            recognition.onresult = (event) => {
                const transcript = event.results[event.results.length - 1][0].transcript.trim();
                if (transcript) {
                    recognition.stop(); // Stop listening to process
                    addMessageToLog(transcript, 'user');
                    processCommand(transcript);
                }
            };
        }

        function startListening() {
            if (!isListening && recognition && !synth.speaking) {
                try {
                    recognition.start();
                } catch (e) {
                    console.error("Could not start recognition:", e);
                    // This can happen if start() is called too soon after stop()
                    setTimeout(() => startListening(), 100);
                }
            }
        }
        
        function updateStatus(message, isError = false) {
            statusEl.textContent = message;
            statusEl.style.color = isError ? '#ff6b6b' : 'var(--text-color)';
        }

        function addMessageToLog(text, sender) {
            const div = document.createElement('div');
            div.textContent = text;
            div.className = `log-entry ${sender}-message`;
            logEl.appendChild(div);
            logEl.scrollTop = logEl.scrollHeight; // Auto-scroll
        }
        
        async function processCommand(command) {
            if (openRouterApiKey === 'YOUR_OPENROUTER_API_KEY' || !openRouterApiKey) {
                speak("My connection to the network is not configured. Please set the OpenRouter API key.");
                return;
            }

            updateStatus("Thinking...");
            visualizerEl.classList.remove('listening');

            try {
                const response = await fetch("https://openrouter.ai/api/v1/chat/completions", {
                    method: "POST",
                    headers: {
                        "Authorization": `Bearer ${openRouterApiKey}`,
                        "Content-Type": "application/json"
                    },
                    body: JSON.stringify({
                        "model": "mistralai/mistral-7b-instruct:free",
                        "messages": [
                            {"role": "system", "content": "You are MAIRA AI. You speak like a calm, intelligent human assistant. No robotic phrases. No repeating words. Provide short, confident replies."},
                            {"role": "user", "content": command}
                        ]
                    })
                });

                if (!response.ok) {
                    throw new Error(`API Error: ${response.statusText}`);
                }

                const data = await response.json();
                const reply = data.choices[0].message.content;
                speak(reply);

            } catch (error) {
                console.error("API Error:", error);
                speak("I'm having trouble connecting to my network right now.");
            }
        }

        function speak(text) {
            if (!text || !synth) {
                console.error("Speech synthesis unavailable.");
                startListening();
                return;
            }
            
            addMessageToLog(text, 'maira');
            updateStatus("Speaking...");
            visualizerEl.classList.add('speaking');
            
            // Break text into natural sentences for better flow
            const sentences = text.match(/[^.!?]+[.!?]*|[^.!?]+$/g) || [text];

            synth.cancel(); // Clear any previous speech queue

            // Add pause before starting to speak
            setTimeout(() => {
                sentences.forEach((sentence, index) => {
                    const utterance = new SpeechSynthesisUtterance(sentence.trim());
                    utterance.voice = maleVoice;
                    utterance.pitch = 0.9;
                    utterance.rate = 1.0;
                    
                    // When the last sentence finishes, restart listening
                    if (index === sentences.length - 1) {
                        utterance.onend = () => {
                           visualizerEl.classList.remove('speaking');
                           updateStatus("Ready for your command.");
                           startListening();
                        };
                    }
                    
                    synth.speak(utterance);
                });
            }, 400); // 300-500ms pause
        }

        document.addEventListener('DOMContentLoaded', initializeMaira);
    </script>
</body>
  </html>
